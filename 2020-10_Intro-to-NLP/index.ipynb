{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.7 64-bit ('base': conda)",
   "display_name": "Python 3.7.7 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "83f783ff8513835ca211262fa46ed730f0352cd4ac126cc088daa5443e987af3"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Introduction to Natural Language Processing (NLP) with spaCy\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This code is from the NLP course available in Kaggle: https://www.kaggle.com/learn/natural-language-processing.\n",
    "\n",
    "I wanted to start it from 0 to make sure I totally understand what's behind.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Part 1 - Intro to NLP\n",
    "\n",
    "In this part, we will focus on 3 main concepts:\n",
    "* Tokenizing\n",
    "* Text Preprocessing\n",
    "* Pattern Matching\n",
    "\n",
    "I will use the leading NLP library: spaCy.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## NLP with spaCy\n",
    "\n",
    "spaCy is the leading library for NLP. To find out more, have a look at its documentation: https://spacy.io/usage.\n",
    "To install, you may need: \n",
    "```\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download en\n",
    "```\n",
    "\n",
    "First of all, we have to import the library and load the english language model.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "source": [
    "Then, with this model loaded, we can now process text:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "This coffe is awful. I shouldn't have taken a big cup!"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "doc = nlp(\"This coffe is awful. I shouldn't have taken a big cup!\")\n",
    "doc"
   ]
  },
  {
   "source": [
    "## Tokenizing\n",
    "\n",
    "A document contains tokens. A token is a simple unit of text, like a word or punctuation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This\ncoffe\nis\nawful\n.\nI\nshould\nn't\nhave\ntaken\na\nbig\ncup\n!\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "source": [
    "## Text preprocessing\n",
    "\n",
    "A token comes with additional information. \n",
    "It has the \"lemma\" of the word. The lemma of a word is its base, its root.\n",
    "For instance \"speak\" is the lemma of \"speaking\".\n",
    "\n",
    "A token may be a stopword. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Token \t\tLemma \t\t Stopword\n----------------------------------------\nThis \t\tthis \t\tTrue\ncoffe \t\tcoffe \t\tFalse\nis \t\tbe \t\tTrue\nawful \t\tawful \t\tFalse\n. \t\t. \t\tFalse\nI \t\t-PRON- \t\tTrue\nshould \t\tshould \t\tTrue\nn't \t\tnot \t\tTrue\nhave \t\thave \t\tTrue\ntaken \t\ttake \t\tFalse\na \t\ta \t\tTrue\nbig \t\tbig \t\tFalse\ncup \t\tcup \t\tFalse\n! \t\t! \t\tFalse\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token \\t\\tLemma \\t\\t Stopword\")\n",
    "print(\"-\"*40)\n",
    "for token in doc:\n",
    "    print(f\"{str(token)} \\t\\t{token.lemma_} \\t\\t{token.is_stop}\")"
   ]
  },
  {
   "source": [
    "By knowing if a token is a stopword and what is its lemma, you can preprocess your text to contain more valuable content."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Pattern Matching\n",
    "\n",
    "You may often need to mach some words. A comme tool for that is to use regular expressions. With spaCy, you can do that easily.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# We specify attr='LOWER' to make the matcher case insensitive\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the terms\n",
    "terms = [\"Galaxy Note\", \"iPhone 11\", \"iPhone XS\", \"Google Pixel\"]\n",
    "\n",
    "# We process the terms through spaCy\n",
    "patterns = [nlp(term) for term in terms]\n",
    "\n",
    "# We add our terms to the matcher\n",
    "matcher.add(\"TerminologyList\", patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(3766102292120407359, 18, 20), (3766102292120407359, 24, 26), (3766102292120407359, 32, 34), (3766102292120407359, 35, 37)]\n"
     ]
    }
   ],
   "source": [
    "review = \"Glowing review overall, and some really interesting side-by-side  photography tests pitting the iPhone 11 Pro against the  Galaxy Note 10 Plus and last yearâ€™s iPhone XS and Google Pixel 3.\"\n",
    "\n",
    "doc = nlp(review)\n",
    "\n",
    "matches = matcher(doc)\n",
    "print(matches)"
   ]
  },
  {
   "source": [
    "The results are the match id, the positions of the start and end of the phrase.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TerminologyList iPhone 11\n"
     ]
    }
   ],
   "source": [
    "match_id, start, end = matches[0]\n",
    "print(nlp.vocab.strings[match_id], doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}